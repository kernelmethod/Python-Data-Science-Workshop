{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building Neural Networks with Keras\n",
    "![LISA logo](https://raw.githubusercontent.com/wshand/Python-Data-Science-Workshop/master/assets/LISA_logo_medium.jpg)\n",
    "\n",
    "This notebook is an introduction to building neural networks with the [Keras](https://keras.io) API.\n",
    "\n",
    "# Table of contents\n",
    "* [Introduction](#introduction)\n",
    "* [What are neural networks?](#intro-neural-nets)\n",
    "* [Adding regularization](#regularization)\n",
    "* [Computer vision: building convolution neural networks for the MNIST dataset](#cnn-mnist)\n",
    "* [Saving and loading models](#saving)\n",
    "* [Visualizing networks with TensorBoard](#tensorboard)\n",
    "* [Additional resources](#additional-resources)\n",
    "\n",
    "# Introduction <a id=\"introduction\"></a>\n",
    "One of the biggest recent developments in machine learning is the usage of [_**artificial neural networks**_](https://en.wikipedia.org/wiki/Artificial_neural_network) (ANNs) as a powerful tool in the data scientist's toolbox. Although neural nets have been around for over 70 years, they've only become popular in the past decade thanks to advancements in algorithms and hardware.\n",
    "\n",
    "On the software side, [backpropagation](https://en.wikipedia.org/wiki/Backpropagation) (aka backwards-mode automatic differentiation), introduced in the 1980s, made it much easier to train neural networks. Later, researchers introduced neural network architectures such as [convolutional networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNNs) and [LSTMs](https://en.wikipedia.org/wiki/Long_short-term_memory) that saw success in tasks such as handwriting recognition.\n",
    "\n",
    "Meanwhile, hardware limitations originally made it difficult to train neural networks, which require more data and computing power than most machine learning methods. However, improvements in processor power and the advent of specialized hardware such as graphics processing units have made training neural nets much faster. Massively parallel computing has also enabled boosts in training time: for instance, Google's [AlphaZero](https://en.wikipedia.org/wiki/AlphaZero) was trained on 5,000 tensor processing units in parallel in just a few hours.\n",
    "\n",
    "In this workshop, we will be using [Keras](https://keras.io) to build our own neural networks. Keras is a high-level interface for building ANNs, with libraries like [TensorFlow](https://www.tensorflow.org) running on the backend.\n",
    "\n",
    "### Note: you will need to run the following code cell every time you restart this notebook\n",
    "(You don't need to read the code in this cell to understand the rest of the workshop.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### RUN THIS CELL BEFORE USING THE REST OF THE NOTEBOOKc\n",
    "###\n",
    "# Set the Keras backend to be TensorFlow\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy             as np\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.datasets              import mnist\n",
    "from keras                       import backend\n",
    "from keras.utils                 import to_categorical\n",
    "from keras.models                import Sequential\n",
    "\n",
    "########## Helper functions\n",
    "def square_axes(ax, data, expansion_factor=1.05):\n",
    "    # Change limits of plot axes to center on the input dataset, and to put the\n",
    "    # x-axis and y-axis on the same scale\n",
    "    m        = np.mean(data)\n",
    "    max_dist = max([np.linalg.norm(u-m) for u in data]) * expansion_factor\n",
    "    lims     = [m-max_dist, m+max_dist]\n",
    "    try:    ax.set_xlim(lims); ax.set_ylim(lims)\n",
    "    except: ax.xlim(lims); ax.ylim(lims)\n",
    "\n",
    "def plot_decision_boundary(X, clf, ax, incr=1, h=.02):\n",
    "    # Plot the decision boundary for a machine learning classifier on 2D data\n",
    "    xmin, xmax = X[:,0].min()-incr, X[:,0].max()+incr\n",
    "    ymin, ymax = X[:,1].min()-incr, X[:,1].max()+incr\n",
    "    xx, yy     = np.meshgrid(np.arange(xmin,xmax,h),np.arange(ymin,ymax,h))\n",
    "    Z          = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=.2)\n",
    "\n",
    "def load_mnist_wrapper(n_classes=10):\n",
    "    # Loads the MNIST dataset, and does some preprocessing\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    # Only keep the first n_classes digits\n",
    "    X_train = X_train[y_train < n_classes]; y_train = y_train[y_train < n_classes]\n",
    "    X_test  = X_test[y_test < n_classes];   y_test  = y_test[y_test < n_classes]\n",
    "    \n",
    "    # Pre-process the data for the backend we're using\n",
    "    if backend.image_data_format() == 'channels_first':\n",
    "        X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "        X_test  = X_test.reshape(X_test.shape[0],   1, 28, 28)\n",
    "    else:\n",
    "        X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "        X_test  = X_test.reshape(X_test.shape[0],   28, 28, 1)\n",
    "\n",
    "    # Change pixel intensities from the range 0 - 255 to the range 0 - 1\n",
    "    X_train = X_train / 255\n",
    "    X_test  = X_test  / 255\n",
    "\n",
    "    # Convert the class labels into a format that Keras will be able to parse\n",
    "    y_train = to_categorical(y_train, n_classes)\n",
    "    y_test  = to_categorical(y_test,  n_classes)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "def lm_vs_ann_helper(x_train, y_train, x_test, y_test, lm, ann):\n",
    "    # Helper function to compare a linear model against a neural net\n",
    "    # on 2D data. Used for section on regularization.\n",
    "    # This function creates two subplots: one for displaying data, and\n",
    "    # one for overlaying predictions on the data.\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    for ax in axes:\n",
    "        ax.scatter(x_train.flatten(), y_train.flatten(), label=\"Training data\")\n",
    "        ax.scatter(x_test.flatten(), y_test.flatten(), label=\"Testing data\")\n",
    "        ax.legend()\n",
    "        \n",
    "    xx = np.linspace(x.min(), x.max(), num=100).reshape((-1, 1))\n",
    "    axes[1].plot(xx.flatten(), ann.predict(xx).flatten(),\n",
    "                 label=\"Neural network\", linewidth=2)\n",
    "    axes[1].plot(xx.flatten(), lm.predict(xx).flatten(),\n",
    "                 label=\"Linear regression\", linewidth=2)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "##########################\n",
    "\n",
    "n_mnist_classes = 3\n",
    "\n",
    "# Pre-load data\n",
    "(X_train_mnist, y_train_mnist), (X_test_mnist, y_test_mnist) = \\\n",
    "    load_mnist_wrapper(n_classes=n_mnist_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are neural networks? <a id=\"intro-neural-nets\"></a>\n",
    "Neural networks are a family of predictive algorithms used in machine learning. The simplest type of neural network, a _feedforward neural net_, has the following components:\n",
    "\n",
    "- **Neurons**: each neuron in a neural network can be thought of as a little function, which takes one or more numbers as inputs and spits out a number as its output.\n",
    "- **Layers**: a layer is a collection of neurons that all take the same inputs. All we're doing in a neural network is taking some initial input and passing it through a chain of layers. We train the neural network so that the final layer contains the predictions we want to make.\n",
    "  - **Input layer**: the first layer in a neural network. This isn't really neural network layer, technically -- all it does is start the process of feeding forward some user-supplied input to the neural net.\n",
    "  - **Hidden layers**: a chain of neural network layers between the input and output layers. These are the layers that are doing most of the computation for the neural net.\n",
    "  - **Output layer**: the final layer of the neural network. We use whatever the output of this layer is as our final prediction.\n",
    "- **Loss function**: a function that takes an input $x$ and the output we want the neural network to predict for that input, $y$. For instance, when we use the MNIST dataset later, $x$ will be a $28\\times 28$-pixel image of a handwritten digit, and $y$ will be a label for the digit in that image (i.e. some value in the range $0$ to $9$). The loss function \"scores\" how close the neural network gets to predicting $y$ from $x$. When we train the neural network, our objective is to make this loss function as small as possible.\n",
    "- **Optimizer**: an algorithm that we use to minimize the loss function. This optimizer \"teaches\" the neural network to make better predictions.\n",
    "\n",
    "Within each neuron, we also have the following:\n",
    "- **Weights**: suppose that a neuron gets some inputs $x_1$, $x_2$, $\\ldots$, $x_n$. We associate each of those inputs with numbers $w_1$, $w_2$, $\\ldots$, $w_n$ that are unique to each neuron, called _weights_. The process of training a neural network involves adjusting these weights to make more and more accurate predictions.\n",
    "- **Activation function**: with each layer we must also associate some nonlinear function, called an _activation function_. The reason for this is that it can be shown that without these activation functions, neural networks only have the predictive power of linear regression, which we probably don't want.\n",
    "\n",
    "ReLU:\n",
    "\n",
    "$$\n",
    "f(x) = \\begin{cases}\n",
    "x \\text{ if } x\\ge 0 \\\\\n",
    "0 \\text{ otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$f(ax + by) = af(x) + bf(y)$\n",
    "\n",
    "Mathematically, a neuron with inputs $x_1$, $\\ldots$, $x_n$, weights $w_1$, $\\ldots$, $w_n$, and activation function $f$ computes\n",
    "\n",
    "$$\n",
    "f\\left(\\sum_{i=1}^n w_ix_i + b\\right) = f\\bigg(w_1x_1 + w_2x_2 + \\ldots + w_nx + b\\bigg)\n",
    "$$\n",
    "\n",
    "Here's a visual representation of what a feedforward neural network looks like:\n",
    "\n",
    "<p style=\"font-size: 11px\"><em><a href=\"https://commons.wikimedia.org/wiki/File:Colored_neural_network.svg#/media/File:Colored_neural_network.svg\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/1200px-Colored_neural_network.svg.png\" alt=\"Colored neural network.svg\" style=\"width: 40%; height: 40%\"></a><br>By <a href=\"//commons.wikimedia.org/wiki/User_talk:Glosser.ca\" title=\"User talk:Glosser.ca\">Glosser.ca</a> - <span class=\"int-own-work\" lang=\"en\">Own work</span>, Derivative of <a href=\"//commons.wikimedia.org/wiki/File:Artificial_neural_network.svg\" title=\"File:Artificial neural network.svg\">File:Artificial neural network.svg</a>, <a href=\"https://creativecommons.org/licenses/by-sa/3.0\" title=\"Creative Commons Attribution-Share Alike 3.0\">CC BY-SA 3.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=24913461\">Link</a></em></p>\n",
    "\n",
    "For the rest of this notebook we'll actually try to create some neural networks with Keras. If you're interested in the theory and training of neural nets, visit the [additional resources section](#additional-resources) at the end of this notebook.\n",
    "\n",
    "We'll build our first neural net by trying to classify points in an example dataset. The dataset consists of two circles of points, one inscribed inside the other. The points on the outer circle are in class 0, and the points on the inner circle are in class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, classes = make_circles(n_samples=200, factor=0.3, noise=0.1)\n",
    "plt.scatter(X[classes == 0,0], X[classes == 0,1], label=\"Class 0\")\n",
    "plt.scatter(X[classes == 1,0], X[classes == 1,1], label=\"Class 1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network will have the following architecture:\n",
    "\n",
    "- One input layer, with two neurons (i.e. two inputs): one for $x$ coordinate, and one for $y$ coordinate.\n",
    "- A hidden layer with $16$ neurons. This layer will use a `relu` activation function.\n",
    "- An output layer with one neuron which provides an estimated probability that a data point is in class 0. This layer will use a `sigmoid` activation function.\n",
    "\n",
    "You can find a full list of available activation functions in the Keras documentation: https://keras.io/activations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of repeatedly calling `model.add()`, you can give Keras the layers you want to have in your neural net when you call `Sequential()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1,  activation='sigmoid')\n",
    "]\n",
    "\n",
    "model = Sequential(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the network! Our first step is to call `model.compile`. The `compile` function allows us to make some configurations before we start training. We can do things like\n",
    "\n",
    "* Choose a [loss function](https://keras.io/losses/), which helps us score how good our neural net is\n",
    "* Set which [optimizer](https://keras.io/optimizers/) we'll use to minimize the loss function\n",
    "* Set options that will be used on the backend (in this case, by TensorFlow) beneath the Keras API.\n",
    "\n",
    "For more information on compilation options, see the [getting started guide](https://keras.io/getting-started/sequential-model-guide/#compilation) or the [Keras documentation](https://keras.io/models/model/#compile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Options:\n",
    "    loss='binary_crossentropy'\n",
    "        - Loss function for classification problems with two classes.\n",
    "        - If we had more than one class, we'd use 'categorical_crossentropy' instead\n",
    "\n",
    "    optimizer='rmsprop'\n",
    "        - Tells Keras to use the RMSProp optimization routine\n",
    "        - Reference: https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\n",
    "\n",
    "    metrics=['accuracy']\n",
    "        - When we're training our neural net, Keras will tell us how accurate the net\n",
    "          currently is at making correct predictions.\n",
    "\"\"\"\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model has been compiled, you can run `model.fit` to actually train the neural network. When you call `model.fit`, you provide a few parameters (such as `epochs` and `steps_per_epoch`) that tell Keras how much training you want to do. In the code below, we do $5$ training epochs each consisting of $512$ training steps, totalling $5\\times 512 = 2560$ steps of neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model.fit takes two numpy arrays. The first is the array X, which contains the actual\n",
    "data. The second is an array y that contains the predictions we are trying to make. Since\n",
    "this is a classification problem with two classes (\"blue\" and \"orange\"), y should contain\n",
    "0's and 1's (where 0 = blue, 1 = orange).\n",
    "\n",
    "Options:\n",
    "    epochs=5\n",
    "        - Each 'epoch' can be thought of as a unit of training, consisting of\n",
    "          many small training steps\n",
    "        - The number of training steps in each epoch is controlled by \n",
    "          steps_per_epoch\n",
    "\n",
    "    steps_per_epoch=512\n",
    "        - Number of training steps to do in each epoch\n",
    "        - The total number of training steps done when model.fit is called is \n",
    "          epochs * steps_per_epoch\n",
    "     \n",
    "    verbose=1\n",
    "        - Tells Keras to show a progress bar for each training epoch that keeps us\n",
    "          updated on how much the network has been trained.\n",
    "\"\"\"\n",
    "model.fit(X, classes, epochs=5, steps_per_epoch=512, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your neural network has been trained, you can use `model.predict()` to predict which class a point is in. `model.evaluate()` tells you the value of the loss function and the neural network's accuracy on a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with model.predict\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Determine loss and accuracy with model.evaluate\n",
    "loss, accuracy = model.evaluate(X, classes)\n",
    "\n",
    "print(\"Training loss: %6.5f\" % loss)\n",
    "print(\"Training accuracy: %6.5f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the \"decision boundary\" learned by our neural network, a graphical representation of where the neural network decides that a point is in class 0 or class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot data, and the decision boundary found by the network\n",
    "\"\"\"\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "for ax in axes:\n",
    "    ax.scatter(X[classes == 0,0], X[classes == 0,1])\n",
    "    ax.scatter(X[classes == 1,0], X[classes == 1,1])\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "# plot_decision_boundary is a function I've created that you can\n",
    "# find in the first code cell in this notebook.\n",
    "plot_decision_boundary(X, model, axes[1], incr=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "The previous task is an example of a classification problem: given some data and some labels (e.g. \"inner circle\" and \"outer circle\"), create a neural network that can look at a new point and predict its label. Regression is another common machine learning task. In a regression problem, you must instead predict a *response variable*, which can take on a continuum of values. For instance, all of the following are regression problems:\n",
    "\n",
    "* How well will a student score on a standardized test given their grades in school?\n",
    "* Can we assess how happy a person is (on a scale of 1 - 10) from a sample of their writing?\n",
    "* Given data about air pressure, temperature, and wind speed, can I predict how much rainfall we will receive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "x = np.random.uniform(-2, 2, size=(80,1))\n",
    "y = np.sin(np.pi * x) + np.random.normal(scale=.15, size=(x.size,1))\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_dim=1),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(8,  activation='relu'),\n",
    "    Dense(1,  activation='linear')  # Use linear activation in last layer for regression,\n",
    "])                                  # sigmoid or tanh for classification when there's only\n",
    "                                    # one class, and softmax when there's multiple classes\n",
    "\n",
    "# Since we're doing regression, we're going to use 'mean_squared_error' as\n",
    "# our loss function\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "model.fit(x, y, steps_per_epoch=256, epochs=4)\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "xx  = np.linspace(x.min(), x.max(), num=100)\n",
    "plt.scatter(x,y)\n",
    "plt.plot(xx, model.predict(xx), color='r', label='Neural net predictions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding regularization <a id=\"regularization\"></a>\n",
    "Neural networks are such a power machine learning tool that they can occasionally be a little _too_ powerful. In the [second workshop of this series](https://nbviewer.jupyter.org/github/wshand/Python-Data-Science-Workshop/blob/master/2.%20Intro%20to%20Machine%20Learning%20in%20Python%20with%20Scikit-learn.ipynb), we discussed how a machine learning method can get a great score with the data it was trained on and still fail in the real world. Neural networks also do well with training data, but -- if improperly constructed -- are poor at making predictions for data they've never seen before.\n",
    "\n",
    "As a demonstration, we're going to try another very simple regression problem, like we did in the previous section. However, we're going to use a much more complicated neural net than necessary, consisting of more neurons and layers than we really need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model    import LinearRegression\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\"\"\"\n",
    "Create some example data\n",
    "\"\"\"\n",
    "x = np.random.uniform(-1, 1, size=(60,1))\n",
    "y = x + np.random.normal(scale=.15, size=x.shape)\n",
    "\n",
    "# Split into training and testing data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
    "\n",
    "\"\"\"\n",
    "Train a complicated neural network on the data\n",
    "\"\"\"\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_dim=1),\n",
    "    Dense(64,  activation='relu'),\n",
    "    Dense(32,  activation='relu'),\n",
    "    Dense(1,   activation='linear')\n",
    "])\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(x_train, y_train, epochs=3, steps_per_epoch=512, verbose=1)\n",
    "\n",
    "\"\"\"\n",
    "Perform linear regression\n",
    "\"\"\"\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(x_train, y_train)\n",
    "\n",
    "\"\"\"\n",
    "Compare the two models\n",
    "\"\"\"\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print('ANN training loss:          %5.4f' % mean_squared_error(y_train, model.predict(x_train)))\n",
    "print('ANN testing loss:           %5.4f' % mean_squared_error(y_test,  model.predict(x_test)))\n",
    "print('Linear model training loss: %5.4f' % mean_squared_error(y_train, linear_model.predict(x_train)))\n",
    "print('Linear model testing loss:  %5.4f' % mean_squared_error(y_test,  linear_model.predict(x_test)))\n",
    "\n",
    "lm_vs_ann_helper(x_train, y_train, x_test, y_test, linear_model, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the output I got on one run of the code cell above:\n",
    "\n",
    "> ```\n",
    "Epoch 1/3\n",
    "512/512 [==============================] - 1s 1ms/step - loss: 0.0215\n",
    "Epoch 2/3\n",
    "512/512 [==============================] - 0s 934us/step - loss: 0.0125\n",
    "Epoch 3/3\n",
    "512/512 [==============================] - 0s 963us/step - loss: 0.0111\n",
    "ANN training loss:          0.0106\n",
    "ANN testing loss:           0.0407\n",
    "Linear model training loss: 0.0215\n",
    "Linear model testing loss:  0.0188```\n",
    "\n",
    "The mean squared error received by the ANN on the data it was trained with ($0.0102$) was almost four times better than the MSE on the testing data ($0.0386$), which it hadn't seen before. Meanwhile, fitting a line to the same training data via linear regression got an error on the testing data of $0.0188$, less than half of what the ANN got on that data.\n",
    "\n",
    "The problem of getting a much better score on the training data than the testing data is known as *overfitting*. Neural networks can be especially prone to overfitting.\n",
    "\n",
    "There are two ways to prevent overfitting your training data:\n",
    "\n",
    "* make your neural net simpler; or\n",
    "* add regularization to your ANN.\n",
    "\n",
    "If your neural net is doing much better on the training data than the testing data, simplifying your network's architecture is the easiest fix to try. In the code cell below, I try a neural net with only two layers and far fewer neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use a much simpler neural net on this data\n",
    "\"\"\"\n",
    "model = Sequential([\n",
    "    Dense(4, activation='relu', input_dim=1),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(x_train, y_train, epochs=3, steps_per_epoch=512, verbose=1)\n",
    "\n",
    "lm_vs_ann_helper(x_train, y_train, x_test, y_test, linear_model, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pattern learned by the simplified neural net in this case is much closer to what we got with the linear model. Now we do much better at making predictions for the data we haven't seen before.\n",
    "\n",
    "However, simplifying your network isn't usually the best way to try to prevent overfitting. First, it's difficult to tell what part of the network you need to simplify. Moreover, it decreases the predictive power of your neural net.\n",
    "\n",
    "A better method is to use _regularization_. A regularizer is a method for controlling the growth of the weights in your neural network as you train it. This is useful because in general overfitting occurs due to various oddities that occur to the weights as you train your ANN:\n",
    "\n",
    "* Some weights grow very large. This usually indicates that your neural net learned some strange feature that only occurred by chance via random noise in the training data.\n",
    "* One layer might overemphasize the importance of a few neurons in the previous layers, and ignore the rest of the neurons.\n",
    "\n",
    "To fix the first problem, we change our loss function from\n",
    "\n",
    "$$\n",
    "\\text{loss function} = \\text{numerical penalty on incorrect predictions}\n",
    "$$\n",
    "\n",
    "to\n",
    "\n",
    "$$\n",
    "\\text{loss function} = \\text{numerical penalty on incorrect predictions} + \\text{penalty on size of weights}\n",
    "$$\n",
    "\n",
    "The most common penalties to apply here are known as $L^1$ and $L^2$ regularization. If $\\mathcal{W}$ is the set of all of the weights in the network, then\n",
    "\n",
    "\\begin{align*}\n",
    "L^1 \\text{ regularization:} & & \\text{penalty on size of weights} & = \\lambda\\sum_{w_i\\in\\mathcal{W}} |w_i| \\\\\n",
    "L^2 \\text{ regularization:} & & \\text{penalty on size of weights} & = \\lambda\\sum_{w_i\\in\\mathcal{W}} |w_i|^2\n",
    "\\end{align*}\n",
    "\n",
    "where $\\lambda > 0$ is some number chosen by the designer of the ANN.\n",
    "\n",
    "To fix the second problem, we can use a regularizer known as _dropout_ ([Hinton et al, 2012](https://arxiv.org/abs/1207.0580)). A dropout layer takes the inputs from the previous layer and randomly sets some fraction of them to zero before passing them to the next layer. This prevents the neural net from only relying on a few neurons in each layer -- the outputs of those neurons could be set to zero by a dropout layer and then the ANN wouldn't be able to use their outputs any more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers       import Dropout\n",
    "from keras.regularizers import l1\n",
    "\n",
    "# Same neural net as before, but with some added regularization\n",
    "model = Sequential([\n",
    "    # Add some L^2 regularization to keep the weights in the first layer down\n",
    "    Dense(128, activation='relu', input_dim=1, activity_regularizer=l1(1e-5)),\n",
    "    \n",
    "    # Randomly drop 50% of the inputs from the previous layer\n",
    "    Dropout(0.5),\n",
    "    Dense(64,  activation='relu', activity_regularizer=l1(1.6e-4)),\n",
    "    \n",
    "    # Randomly drop 50% of the inputs from the previous layer\n",
    "    Dropout(0.5),\n",
    "    Dense(32,  activation='relu', activity_regularizer=l1(8e-5)),\n",
    "    \n",
    "    # By suggestion of Hinton, don't apply dropout to the output layer\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(x_train, y_train, epochs=3, steps_per_epoch=512, verbose=1)\n",
    "\n",
    "lm_vs_ann_helper(x_train, y_train, x_test, y_test, linear_model, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural net is still overfitting a little bit, but clearly it's doing better than the original version.\n",
    "\n",
    "## Aside: choosing hyperparameters\n",
    "A _hyperparameter_ is a number that must be chosen by the creator of an ML system that affects how the system learns or is designed. Neural networks have an especially large number of hyperparameters -- you have to choose the number of layers, the size of layers, what activation functions should be used in each layer, how much regularization/dropout to apply, and so on.\n",
    "\n",
    "A major downside of neural nets is that an ANN's designer has to adjust each of these parameters until they get a reasonably strong machine learner. The number of hyperparameters can also be beneficial: it allows the designer to make the neural net as powerful as they'd like. But even then, the designer still has to find a way to choose good hyperparameters for the network.\n",
    "\n",
    "* **Suggestions from papers**: the most common way to choose hyperparameters (at least initially) is just to see what experts suggest from past papers. For instance, in his paper introducing dropout, Geoffrey Hinton recommends a dropout rate of $50\\%$. There are also papers such as [\"Practical recommendations for gradient-based training of deep architectures\"](https://arxiv.org/abs/1206.5533v2) by Yoshua Bengio that look at what various heuristics and practical experience say about how you should choose hyperparameters.\n",
    "* **Manual guess-and-check**: it's easy to adjust some of the parameters, re-train your network, and see how much better or worse your ANN is doing. This isn't always a very fast or effective method, though.\n",
    "* **Experience**: in some problem domains there may be specific values of hyperparameters that often work better than others. With enough experience building neural nets for this domain it becomes a lot easier to make good guesses about what hyperparameters you should choose.\n",
    "* **Randomized and grid search**: you can automate the process of hyperparameter selection by choosing some candidate values of each hyperparameter and then iteratively re-training and scoring your neural net on every possible selection of parameters. This is known as _grid search_. You can also just choose some random group of the candidate values, train the ANN, and repeat a few times. This is called _randomized search_. Below I've written some code that uses scikit-learn's `RandomizedSearchCV` class to find good hyperparameters for the regression problem in the last section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A scikit-learn class that helps us choose hyperparameters\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Wraps around a neural network so that we can use the scikit-learn API\n",
    "# to perform grid search.\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "def create_model(l1_1=0, l1_2=0, l1_3=0):\n",
    "    # Clear past TensorFlow sessions to prevent GridSearchCV from using too much memory\n",
    "    # Reference: https://stackoverflow.com/a/42047606\n",
    "    if backend.backend() == 'tensorflow':\n",
    "        backend.clear_session()\n",
    "    \n",
    "    # Same neural net as before, but with some added regularization\n",
    "    model = Sequential([\n",
    "        # Add some L^1 regularization to keep the weights in the first layer down\n",
    "        Dense(128, activation='relu', input_dim=1, activity_regularizer=l1(l1_1)),\n",
    "\n",
    "        # Randomly drop 50% of the inputs from the previous layer\n",
    "        Dropout(0.5),\n",
    "        Dense(64,  activation='relu', activity_regularizer=l1(l1_2)),\n",
    "\n",
    "        # Randomly drop 50% of the inputs from the previous layer\n",
    "        Dropout(0.5),\n",
    "        Dense(32,  activation='relu', activity_regularizer=l1(l1_3)),\n",
    "\n",
    "        # By suggestion of Hinton, don't apply dropout to the output layer\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "clf = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "l1_params = [2**n * 1e-5 for n in range(8)]\n",
    "param_grid = dict(l1_1=l1_params, l1_2=l1_params, l1_3=l1_params)\n",
    "\n",
    "# Apply grid search\n",
    "grid = RandomizedSearchCV(clf, param_distributions=param_grid, n_jobs=4, cv=3,\n",
    "                          verbose=1, n_iter=30)\n",
    "grid.fit(x,y)\n",
    "\n",
    "# What were the best hyperparameters that we found?\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer vision: building convolutional networks for the MNIST dataset <a id=\"cnn-mnist\"></a>\n",
    "Let's start looking at how neural networks are used for analyzing image data. We're going to use the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database), a classic computer vision dataset containing images of tens of thousands of handwritten digits. The MNIST dataset can actually be loaded into Python using Keras:\n",
    "\n",
    "```python\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "```\n",
    "\n",
    "We will build a neural network that will be able to look at an MNIST digit tell us what number it sees. To help shorten training times we're only going to look at the digits $0$, $1$, and $2$; you can build a network that classifies more digits by increasing `n_mnist_classes` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "### For purpose of demonstration we're not going to use all of  ########\n",
    "### the digits. Increase n_mnist classes if you want to try     ########\n",
    "### training your neural net on more types of digits.           ########\n",
    "########################################################################\n",
    "\n",
    "n_mnist_classes = 3\n",
    "\n",
    "########################################################################\n",
    "########################################################################\n",
    "\n",
    "# load_mnist_wrapper brings in the MNIST data with keras.datasets.mnist.load_data()\n",
    "# and does some useful preprocessing. Check out the first code cell of this notebook\n",
    "# to see how this works.\n",
    "(X_train_mnist, y_train_mnist), (X_test_mnist, y_test_mnist) = \\\n",
    "    load_mnist_wrapper(n_classes=n_mnist_classes)\n",
    "    \n",
    "# Show a few randomly selected images\n",
    "fig, axes = plt.subplots(3, 5, figsize=(8,8))\n",
    "\n",
    "c = np.random.choice(X_train_mnist.shape[0], axes.size)\n",
    "digits, classes = X_train_mnist[c], y_train_mnist[c]\n",
    "\n",
    "for (ax,img,num) in zip(axes.flatten(), digits, classes):\n",
    "    ax.imshow(img.reshape((28,28)), cmap='gray')\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.set_xlabel(\"Digit: \" + str(num.argmax()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could apply a plain feedforward neural network to this dataset, and you'd get decent results. However, the state-of-the-art in computer vision uses [*convolutional neural networks* (CNNs)](https://en.wikipedia.org/wiki/Convolutional_neural_network), a type of neural network with a slightly different architecture from a feedforward network. Here's what a typical convolutional neural network looks like:\n",
    "\n",
    "<p style=\"font-size: 0.8em; font-weight: italic;\">\n",
    "    <a href=\"https://commons.wikimedia.org/wiki/File:Typical_cnn.png#/media/File:Typical_cnn.png\">\n",
    "        <img src=\"https://upload.wikimedia.org/wikipedia/commons/6/63/Typical_cnn.png\" alt=\"Typical cnn.png\">\n",
    "    </a>\n",
    "    <br>\n",
    "    <em>\n",
    "    By <a href=\"//commons.wikimedia.org/w/index.php?title=User:Aphex34&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"User:Aphex34 (page does not exist)\">Aphex34</a> - <span class=\"int-own-work\" lang=\"en\">Own work</span>, <a href=\"https://creativecommons.org/licenses/by-sa/4.0\" title=\"Creative Commons Attribution-Share Alike 4.0\">CC BY-SA 4.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=45679374\">Link</a>\n",
    "    </em>\n",
    "</p>\n",
    "\n",
    "A CNN has some additional types of layers that you wouldn't see in a standard feedforward network:\n",
    "- **Convolutional layers**: a convolutional layer consists of a set of \"convolutional kernels\", each of which is associated with a \"window\". A $3\\times 3$ kernel, for instance, is associated with a $3\\times 3$-pixel patch in the image. The kernel takes all of the pixels in that patch, creates a weighted sum of them, and then gives that sum to an activation function, much like how a feedforward network's neuron works. However, unlike a neuron, we slide that kernel across rows of the image, computing the value of that kernel across many $3\\times 3$-pixel patches and computing its output for each patch.\n",
    "- **Pooling layers**: Another important component of CNNs is *pooling*. After each layer of convolutions, we follow up with a pooling layer. In a pooling layer, we partition the output of the convolutional layer into a lot of different patches, and the calculate a function over those patches. These functions are things like finding the maximum value of each patch, and replacing the patch by that maximum value (\"max pooling\").\n",
    "\n",
    "Convolutional neural networks tend to be more effective in computer vision than feedforward networks because they consider spatial locality. Because of the nature of image data, the values in each patch considered by a convolutional layer or pooling layer tend to be highly correlated with one another, often representing distinct features of the image. The convolutional layers learn these features, and uses them to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras.utils  import to_categorical\n",
    "from keras        import backend\n",
    "\n",
    "# Slight modification of architecture from the following example in the\n",
    "# Keras repository:\n",
    "# https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
    "mnist_model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3,3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Conv2D(32, (3,3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(.25),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(.5),\n",
    "    Dense(n_mnist_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "mnist_model.compile(loss='categorical_crossentropy', optimizer='adadelta',\n",
    "                    metrics=['accuracy'])\n",
    "mnist_model.fit(X_train_mnist, y_train_mnist, verbose=1, batch_size=256,\n",
    "                epochs=1, validation_data=(X_test_mnist, y_test_mnist))\n",
    "\n",
    "score = mnist_model.evaluate(X_test_mnist, y_test_mnist, verbose=0)\n",
    "print('Test loss: %.4f'     % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])\n",
    "\n",
    "# If we didn't get 100% classification accuracy, show some images that we\n",
    "# misclassified\n",
    "predictions = mnist_model.predict(X_test_mnist).argmax(axis=1)\n",
    "mclf_idx    = (predictions != y_test_mnist.argmax(axis=1)).flatten()\n",
    "X_mclf      = X_test_mnist[mclf_idx]\n",
    "y_mclf      = y_test_mnist[mclf_idx]\n",
    "pred_mclf   = predictions[mclf_idx]\n",
    "\n",
    "print(\"On test set, misclassified %d out of %d\" % (X_mclf.shape[0], predictions.size))\n",
    "\n",
    "if X_mclf.shape[0] >= 3:\n",
    "    fig, axes   = plt.subplots(1,3,figsize=(8,8))\n",
    "    c           = np.random.choice(X_mclf.shape[0], 3, replace=False)\n",
    "    for (ii,ax) in enumerate(axes):\n",
    "        ax.imshow(X_mclf[c[ii]].reshape((28,28)), cmap='gray')\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        pred_class, true_class = pred_mclf[c[ii]], y_mclf[c[ii]].argmax()\n",
    "        ax.set_xlabel(\"Predicted: \" + str(pred_class) + \n",
    "                      \"\\nTrue class: \" + str(true_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading models <a id=\"saving\"></a>\n",
    "Neural networks are computationally expensive and require a large amount of time to train. The largest nets can take days to train on hundreds or thousands of specialized hardware units running in parallel. As a result, once you've trained a neural net, you may want to save it so that you can use it later on without having to re-train the network.\n",
    "\n",
    "Keras allows you to save an ANN to a file and then reload it later on with the `save()` function. When you call `model.save(filename)` Keras stores `model` in an HDF5 file with all the details of the network, including\n",
    "\n",
    "* the ANN's architecture (i.e. number of layers, nodes per layer, etc.);\n",
    "* the weights you found by running `model.fit`; and\n",
    "* the training configuration from `model.compile` (such as what loss function and optimizer you're using)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper functions ###################################\n",
    "def plot_circle_anns(X, classes, model, loaded_model):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(9,5))\n",
    "    for (ax,ann) in zip(axes,(model,loaded_model)):\n",
    "        ax.scatter(X[classes == 0,0], X[classes == 0,1]); ax.set_xticks([])\n",
    "        ax.scatter(X[classes == 1,0], X[classes == 1,1]); ax.set_yticks([])\n",
    "        plot_decision_boundary(X, ann, ax, incr=0.1)\n",
    "    axes[0].set_title(\"Original model\")\n",
    "    axes[1].set_title(\"Model loaded from file\")\n",
    "    plt.show()\n",
    "#########################################################\n",
    "\n",
    "from keras.models import load_model\n",
    "    \n",
    "X, classes = make_circles(n_samples=200, factor=0.3, noise=0.1)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.fit(X, classes, epochs=5, steps_per_epoch=512)\n",
    "\n",
    "\"\"\"\n",
    "Save the ANN as an HDF5 (.h5) file\n",
    "\"\"\"\n",
    "model.save('keras_circles_ann.h5')\n",
    "\n",
    "\"\"\"\n",
    "Now load model from disk\n",
    "\"\"\"\n",
    "loaded_model = load_model('keras_circles_ann.h5')\n",
    "\n",
    "\"\"\"\n",
    "Show decision boundaries for both ANNs side-by-side, overlaid\n",
    "on the dataset\n",
    "\"\"\"\n",
    "plot_circle_anns(X, classes, model, loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything worked correctly, the left and right plots should be identical.\n",
    "\n",
    "`model.save` will also save the current training state of the network. This allows you to spend time training a network with `model.fit`, save it, and then come back later and continue training the model. All you have to do is call the `fit` function again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do five more epochs of training with the model that was loaded\n",
    "# from the save file\n",
    "loaded_model.fit(X, classes, epochs=5, steps_per_epoch=512)\n",
    "\n",
    "plot_circle_anns(X, classes, model, loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to use `keras.models.load_model` is to share neural nets with others. For instance, I trained a neural net for the entire MNIST dataset (all 10 digits) using [some example code](https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py) and added it to the GitHub repository for this workshop. The code cell below downloads this model from the repository (if it isn't already downloaded) and scores it on the MNIST test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "### This code is just to download the model off the ############\n",
    "### GitHub repository if you don't already have it  ############\n",
    "################################################################\n",
    "import shutil\n",
    "from urllib.request import urlopen\n",
    "\n",
    "DOWNLOAD_PATH = os.path.join(os.getcwd(), 'mnist_model.h5')\n",
    "REPO_PATH     = os.path.join(os.getcwd(), 'assets', 'models', 'mnist_model.h5')\n",
    "url           = 'https://github.com/wshand/Python-Data-Science-Workshop/blob/'\\\n",
    "                'master/assets/models/mnist_model.h5?raw=true'\n",
    "\n",
    "if not os.path.isfile(REPO_PATH) and not os.path.isfile(DOWNLOAD_PATH):\n",
    "    with urlopen(url) as response, open(DOWNLOAD_PATH, 'wb') as f:\n",
    "        print('Downloading model from', url)\n",
    "        print('Downloading to', DOWNLOAD_PATH)\n",
    "        shutil.copyfileobj(response, f)\n",
    "################################################################\n",
    "\n",
    "(X_train_mnist, y_train_mnist), (X_test_mnist, y_test_mnist) = load_mnist_wrapper(n_classes=10)\n",
    "if os.path.isfile(REPO_PATH):\n",
    "    model = load_model(REPO_PATH)\n",
    "else:\n",
    "    model = load_model(DOWNLOAD_PATH)\n",
    "\n",
    "score = model.evaluate(X_test_mnist, y_test_mnist, verbose=0)\n",
    "print('Test loss (all 10 digits): %.4f'     % score[0])\n",
    "print('Test accuracy (all 10 digits): %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing networks with TensorBoard <a id=\"tensorboard\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "\"\"\"\n",
    "Clear TensorFlow models that we've made so far\n",
    "\"\"\"\n",
    "backend.clear_session()\n",
    "\n",
    "\"\"\"\n",
    "Create directory for storing TensorBoard log files, if it doesn't exist already.\n",
    "If it does, clear all logs currently in the directory.\n",
    "\"\"\"\n",
    "LOG_DIR=os.path.join(os.getcwd(), 'ann_keras_log_dir')\n",
    "print(\"Using\", LOG_DIR, \"as directory to store TensorBoard logs\")\n",
    "if not os.path.isdir(LOG_DIR):\n",
    "    os.mkdir(LOG_DIR)\n",
    "\n",
    "for f in os.listdir(LOG_DIR):\n",
    "    file_path = os.path.join(LOG_DIR, f)\n",
    "    try:\n",
    "        if os.path.isfile(file_path):\n",
    "            os.unlink(file_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\"\"\"\n",
    "Create circles data\n",
    "\"\"\"\n",
    "X, classes = make_circles(n_samples=200, factor=0.3, noise=0.1)\n",
    "\n",
    "\"\"\"\n",
    "Build neural network\n",
    "\"\"\"\n",
    "model = Sequential([\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "tboard = TensorBoard(log_dir=LOG_DIR, histogram_freq=0,\n",
    "                    write_graph=True, write_images=True)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "\"\"\"\n",
    "Fit the model. Use a TensorBoard instance as a callback so that we can track\n",
    "training over time.\n",
    "\"\"\"\n",
    "model.fit(X, classes, epochs=5, steps_per_epoch=512, verbose=0, callbacks=[tboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start a TensorBoard server, we would usually use the `tensorboard` command in our terminal. So for instance, if `LOG_DIR='/home/ann_keras_log_dir'` in the code above, I would go to my terminal/command line and write\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir /home/ann_keras_log_dir\n",
    "```\n",
    "\n",
    "to start TensorBoard. For convenience, I've added some Python code below that will do this for you. Run the following code cell and then go visit http://localhost:6006 in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start TensorBoard server\n",
    "from tensorboard import program, default\n",
    "\n",
    "tb = program.TensorBoard(default.get_plugins(), program.get_default_assets_zip_provider())\n",
    "tb.configure(argv=[None, '--logdir', LOG_DIR])\n",
    "tb.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional resources <a id=\"additional-resources\"></a>\n",
    "* [Keras documentation](https://keras.io/)\n",
    "* [playground.tensorflow.org](https://playground.tensorflow.org/) allows you to experiment with some simple neural nets.\n",
    "* [Hacker's guide to Neural Networks](https://karpathy.github.io/neuralnets/)\n",
    "* [Neural Networks and Deep Learning (online book)](http://neuralnetworksanddeeplearning.com/)\n",
    "* [Deep Learning](http://www.deeplearningbook.org/) -- textbook by Ian Goodfellow, Yoshua Bengio, and Aaron Courville, who are among the top researchers in neural nets\n",
    "* References for specific ANN architectures:\n",
    "  * Convolutional networks\n",
    "  * LSTMs\n",
    "    * [Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)  \n",
    "* [Practical recommendations for gradient-based training of deep architectures](https://arxiv.org/abs/1206.5533v2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
